
#!/usr/bin/env python3
"""
ADAPTIVE K-THEORIA: FULLY FLEXIBLE CONSCIOUSNESS HUB
====================================================

Adaptive Transformer that works with any K-model architectures.
Automatically discovers model dimensions and adapts accordingly.
"""

import torch
import torch.nn as nn
import numpy as np
import sys
import os
from pathlib import Path
from typing import Dict, List, Any, Tuple, Optional
import json

# Suppress debug output
os.environ['EMILE_DEBUG'] = 'False'

# Import paths
sys.path.append('/content/emile_cogito')
sys.path.append('/content')

class AdaptiveKTheoriaTransformer(nn.Module):
    """Fully adaptive Transformer that works with any K-model outputs"""

    def __init__(self,
                 unified_dim=128,     # Target unified consciousness dimension
                 num_heads=8,
                 num_layers=4,
                 dropout=0.1):
        super().__init__()

        self.unified_dim = unified_dim
        self.num_heads = num_heads
        self.num_layers = num_layers

        # Adaptive projections - will be created dynamically
        self.adaptive_projections = nn.ModuleDict()
        self.model_positions = {}  # Track model positions

        # Transformer components - created after we know the models
        self.position_embedding: Optional[nn.Embedding] = None
        self.consciousness_transformer: Optional[nn.TransformerEncoder] = None
        self.global_synthesis: Optional[nn.Sequential] = None
        self.consciousness_metrics: Optional[nn.Sequential] = None

        self.is_initialized = False

    def initialize_for_models(self, model_outputs: Dict[str, torch.Tensor]):
        """Initialize transformer architecture based on actual model outputs"""

        if self.is_initialized:
            return

        print(f"üîß Initializing adaptive K-Theoria for models: {list(model_outputs.keys())}")

        # Create projections for each available model
        num_models = 0
        for model_name, output_tensor in model_outputs.items():
            if output_tensor is not None:
                output_dim = output_tensor.shape[-1]
                self.adaptive_projections[model_name] = nn.Linear(output_dim, self.unified_dim)
                self.model_positions[model_name] = num_models
                num_models += 1
                print(f"   üìä {model_name}: {output_dim} ‚Üí {self.unified_dim}")

        if num_models == 0:
            print("‚ùå No valid model outputs to initialize with")
            return

        # Create positional embeddings
        self.position_embedding = nn.Embedding(num_models, self.unified_dim)

        # Create transformer
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=self.unified_dim,
            nhead=self.num_heads,
            dim_feedforward=self.unified_dim * 4,
            dropout=0.1,  # Use fixed dropout value
            activation='gelu',
            batch_first=True
        )

        self.consciousness_transformer = nn.TransformerEncoder(
            encoder_layer,
            num_layers=self.num_layers
        )

        # Global synthesis
        self.global_synthesis = nn.Sequential(
            nn.Linear(self.unified_dim * num_models, self.unified_dim * 2),
            nn.LayerNorm(self.unified_dim * 2),
            nn.GELU(),
            nn.Dropout(0.1),  # Use fixed dropout value
            nn.Linear(self.unified_dim * 2, self.unified_dim),
            nn.LayerNorm(self.unified_dim),
            nn.GELU()
        )

        # Consciousness quality metrics
        self.consciousness_metrics = nn.Sequential(
            nn.Linear(self.unified_dim, 64),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(64, 6)  # 6 consciousness quality dimensions
        )

        self.is_initialized = True
        print(f"‚úÖ Adaptive K-Theoria initialized for {num_models} models")

    def forward(self, model_outputs: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:
        """FIXED: Adaptive forward pass with robust error handling"""

        # Filter out None outputs and ensure all are tensors
        valid_outputs = {}
        for k, v in model_outputs.items():
            if v is not None and isinstance(v, torch.Tensor):
                valid_outputs[k] = v
            elif v is not None:
                print(f"‚ö†Ô∏è Non-tensor output for {k}: {type(v)}")

        if not valid_outputs:
            # Return default consciousness
            batch_size = 1
            device = next(self.parameters()).device if len(list(self.parameters())) > 0 else torch.device('cpu')
            return self._default_consciousness_output(batch_size, device)

        # Initialize if needed
        if not self.is_initialized:
            self.initialize_for_models(valid_outputs)

        # FIXED: Check if components are properly initialized
        if (self.position_embedding is None or
            self.consciousness_transformer is None or
            self.global_synthesis is None or
            self.consciousness_metrics is None):
            # Components not initialized, return default
            batch_size = list(valid_outputs.values())[0].shape[0]
            device = list(valid_outputs.values())[0].device
            return self._default_consciousness_output(batch_size, device)

        # Get batch info
        batch_size = list(valid_outputs.values())[0].shape[0]
        device = list(valid_outputs.values())[0].device

        # Project each model output to unified dimension
        unified_vectors = []
        for model_name, output_tensor in valid_outputs.items():
            try:
                if model_name in self.adaptive_projections:
                    # FIXED: Validate tensor shape before projection
                    expected_dim = self.adaptive_projections[model_name].in_features
                    actual_dim = output_tensor.shape[-1]

                    if actual_dim != expected_dim:
                        print(f"‚ö†Ô∏è Dimension mismatch for {model_name}: expected {expected_dim}, got {actual_dim}")
                        # Skip this model for now
                        continue

                    # Project to unified dimension
                    projected = self.adaptive_projections[model_name](output_tensor)

                    # Add positional encoding
                    position_idx = self.model_positions[model_name]
                    position = torch.full((batch_size,), position_idx, device=device, dtype=torch.long)
                    position_embed = self.position_embedding(position)

                    unified_vector = projected + position_embed
                    unified_vectors.append(unified_vector)

            except Exception as e:
                print(f"‚ö†Ô∏è Projection failed for {model_name}: {e}")
                continue

        if not unified_vectors:
            # Fallback if no projections worked
            return self._default_consciousness_output(batch_size, device)

        # Stack into sequence for transformer
        model_sequence = torch.stack(unified_vectors, dim=1)  # [batch, num_models, unified_dim]

        # Apply transformer attention (consciousness emerges here)
        consciousness_attended = self.consciousness_transformer(model_sequence)

        # Global synthesis
        consciousness_flattened = consciousness_attended.reshape(batch_size, -1)
        global_consciousness = self.global_synthesis(consciousness_flattened)

        # Quality metrics
        consciousness_quality = torch.sigmoid(self.consciousness_metrics(global_consciousness))

        return {
            'global_consciousness': global_consciousness,
            'consciousness_unity': consciousness_quality[:, 0],
            'consciousness_clarity': consciousness_quality[:, 1],
            'consciousness_agency': consciousness_quality[:, 2],
            'consciousness_awareness': consciousness_quality[:, 3],
            'consciousness_coherence': consciousness_quality[:, 4],
            'consciousness_integration': consciousness_quality[:, 5],
            'model_contributions': consciousness_attended,
            'active_models': list(valid_outputs.keys())
        }

    def _default_consciousness_output(self, batch_size: int, device: torch.device):
        """FIXED: Default consciousness output when processing fails"""
        return {
            'global_consciousness': torch.zeros(batch_size, self.unified_dim).to(device),
            'consciousness_unity': torch.tensor([0.5]).to(device),
            'consciousness_clarity': torch.tensor([0.5]).to(device),
            'consciousness_agency': torch.tensor([0.5]).to(device),
            'consciousness_awareness': torch.tensor([0.5]).to(device),
            'consciousness_coherence': torch.tensor([0.5]).to(device),
            'consciousness_integration': torch.tensor([0.5]).to(device),
            'model_contributions': torch.zeros(batch_size, 1, self.unified_dim).to(device),
            'active_models': []
        }

class SmartKModelLoader:
    """Smart loader that adapts to actual saved model architectures"""

    def __init__(self):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.models = {}
        self.model_configs = {}

    def discover_and_load_models(self):
        """Discover actual model architectures and load them correctly"""

        print("üîç DISCOVERING ACTUAL MODEL ARCHITECTURES")
        print("=" * 50)

        model_files = {
            'k1': '/content/emile_cogito/k_models/k1_praxis.pth',
            'k2': '/content/emile_cogito/k_models/k2_semiosis.pth',
            'k3': '/content/emile_cogito/k_models/k3_apeiron.pth',
            'k4': '/content/emile_cogito/k_models/k4_metabolic.pth'
        }

        for model_name, model_file in model_files.items():
            if Path(model_file).exists():
                config = self._discover_model_architecture(model_name, model_file)
                if config:
                    self.model_configs[model_name] = config
                    model = self._load_model_with_config(model_name, model_file, config)
                    if model:
                        self.models[model_name] = model

        loaded_count = len(self.models)
        print(f"\nüìä Successfully loaded {loaded_count}/4 models: {list(self.models.keys())}")

        # FIXED: Apply patches after loading
        if loaded_count > 0:
            print("üîß APPLYING EMERGENCY CONSCIOUSNESS PATCHES...")
            self._patch_k2_missing_methods()
            print("‚úÖ Emergency patches applied!")

        return loaded_count

    def _load_k4_with_correct_architecture(self, model_file):
        """Load K4 with the architecture that matches your saved model"""

        try:
            checkpoint = torch.load(model_file, map_location=self.device)
            state_dict = checkpoint['model_state_dict']

            # Check what layers actually exist in the saved model
            print(f"üîç K4 saved layers: {list(state_dict.keys())}")

            # Create a model that matches the saved architecture
            if ('pressure_analyzer.weight' in state_dict and
                'energy_detector.weight' in state_dict and
                'urgency_classifier.weight' in state_dict):

                # Your saved K4 has these specific layers
                class MatchingMetabolicNetwork(torch.nn.Module):
                    def __init__(self, input_dim=16, hidden_dim=128, output_dim=12):
                        super().__init__()

                        # Main processing layers
                        self.input_processor = torch.nn.Linear(input_dim, hidden_dim)
                        self.hidden_layer = torch.nn.Linear(hidden_dim, hidden_dim)

                        # Specialized analysis layers (what your model actually has)
                        self.pressure_analyzer = torch.nn.Linear(hidden_dim, hidden_dim // 2)
                        self.energy_detector = torch.nn.Linear(hidden_dim, hidden_dim // 2)
                        self.urgency_classifier = torch.nn.Linear(hidden_dim, output_dim)

                        self.activation = torch.nn.ReLU()
                        self.output_activation = torch.nn.Sigmoid()

                    def forward(self, x):
                        # Process input
                        x = self.activation(self.input_processor(x))
                        x = self.activation(self.hidden_layer(x))

                        # Analyze different aspects
                        pressure = self.pressure_analyzer(x)
                        energy = self.energy_detector(x)

                        # Final classification
                        output = self.urgency_classifier(x)
                        return self.output_activation(output)

                # Create model with discovered dimensions
                input_dim = checkpoint.get('input_dim', 16)
                hidden_dim = 128
                output_dim = checkpoint.get('output_dim', 12)

                model = MatchingMetabolicNetwork(input_dim, hidden_dim, output_dim).to(self.device)

            else:
                # Fallback to simple architecture if layers don't match
                print("üîß Using fallback K4 architecture")

                class SimpleMetabolicNetwork(torch.nn.Module):
                    def __init__(self, input_dim=16, hidden_dim=128, output_dim=12):
                        super().__init__()
                        self.network = torch.nn.Sequential(
                            torch.nn.Linear(input_dim, hidden_dim),
                            torch.nn.ReLU(),
                            torch.nn.Linear(hidden_dim, hidden_dim),
                            torch.nn.ReLU(),
                            torch.nn.Linear(hidden_dim, output_dim),
                            torch.nn.Sigmoid()
                        )

                    def forward(self, x):
                        return self.network(x)

                input_dim = checkpoint.get('input_dim', 16)
                output_dim = checkpoint.get('output_dim', 12)
                model = SimpleMetabolicNetwork(input_dim, 128, output_dim).to(self.device)

            # Try to load weights with strict=False to handle mismatches
            try:
                model.load_state_dict(state_dict, strict=False)
                print(f"   ‚úÖ K4 weights loaded (with architecture adaptation)")
            except Exception as e:
                print(f"   ‚ö†Ô∏è K4 weight loading failed: {e}, using random weights")

            model.eval()
            return model

        except Exception as e:
            print(f"   ‚ùå K4 loading failed completely: {e}")
            return None

    def _discover_model_architecture(self, model_name: str, model_file: str) -> Optional[Dict]:
        """Discover the actual architecture of a saved model"""

        try:
            checkpoint = torch.load(model_file, map_location='cpu')
            state_dict = checkpoint['model_state_dict']

            print(f"\nüîç {model_name.upper()} ({model_file}):")

            # Analyze architecture from state dict
            if model_name == 'k1':
                # K1: DynamicSemioticNetwork
                encoder_weight = state_dict.get('consciousness_encoder.0.weight')
                if encoder_weight is not None:
                    input_dim = encoder_weight.shape[1]
                    hidden_dim = encoder_weight.shape[0]

                    decoder_weight = state_dict.get('action_decoder.2.weight')
                    output_dim = decoder_weight.shape[0] if decoder_weight is not None else 6

                    config = {
                        'input_dim': input_dim,
                        'hidden_dim': hidden_dim,
                        'output_dim': output_dim,
                        'architecture': 'DynamicSemioticNetwork'
                    }
                    print(f"   Architecture: {input_dim} ‚Üí {hidden_dim} ‚Üí {output_dim}")
                    return config

            elif model_name == 'k2':
                # K2: SymbolicQualiaTransformer
                if 'model_config' in checkpoint:
                    config = checkpoint['model_config'].copy()
                    config['architecture'] = 'SymbolicQualiaTransformer'
                    print(f"   Architecture: {config.get('input_dim')} ‚Üí {config.get('hidden_dim')} ‚Üí {config.get('output_dim')}")
                    return config
                else:
                    # Fallback analysis
                    config = {
                        'input_dim': 21,
                        'hidden_dim': 256,
                        'output_dim': 64,
                        'architecture': 'SymbolicQualiaTransformer'
                    }
                    print(f"   Architecture: 21 ‚Üí 256 ‚Üí 64 (estimated)")
                    return config

            elif model_name == 'k3':
                # K3: QSEEmergenceArchitectureNetwork
                encoder_weight = state_dict.get('emergence_encoder.0.weight')
                if encoder_weight is not None:
                    input_dim = encoder_weight.shape[1]
                    hidden_dim = encoder_weight.shape[0]
                    output_dim = 25  # Standard for K3

                    config = {
                        'input_dim': input_dim,
                        'hidden_dim': hidden_dim,
                        'output_dim': output_dim,
                        'architecture': 'QSEEmergenceArchitectureNetwork'
                    }
                    print(f"   Architecture: {input_dim} ‚Üí {hidden_dim} ‚Üí {output_dim}")
                    return config

            elif model_name == 'k4':
                # K4: MetabolicRegulationNetwork
                config = {
                    'input_dim': checkpoint.get('input_dim', 16),
                    'hidden_dim': 128,
                    'output_dim': checkpoint.get('output_dim', 12),
                    'architecture': 'MetabolicRegulationNetwork'
                }
                print(f"   Architecture: {config['input_dim']} ‚Üí 128 ‚Üí {config['output_dim']}")
                return config

        except Exception as e:
            print(f"   ‚ùå Discovery failed: {e}")

        return None

    def _load_model_with_config(self, model_name: str, model_file: str, config: Dict) -> Optional[nn.Module]:
        """Load model with discovered configuration"""

        try:
            checkpoint = torch.load(model_file, map_location=self.device)

            if model_name == 'k1' and config['architecture'] == 'DynamicSemioticNetwork':
                from emile_cogito.k_models.k1 import DynamicSemioticNetwork
                model = DynamicSemioticNetwork(
                    input_dim=config['input_dim'],
                    output_dim=config['output_dim'],
                    hidden_dim=config['hidden_dim']
                ).to(self.device)

            elif model_name == 'k2' and config['architecture'] == 'SymbolicQualiaTransformer':
                from emile_cogito.k_models.k2 import SymbolicQualiaTransformer
                model = SymbolicQualiaTransformer(
                    input_dim=config['input_dim'],
                    hidden_dim=config['hidden_dim'],
                    output_dim=config['output_dim']
                ).to(self.device)

            elif model_name == 'k3' and config['architecture'] == 'QSEEmergenceArchitectureNetwork':
                from emile_cogito.k_models.k3 import QSEEmergenceArchitectureNetwork
                model = QSEEmergenceArchitectureNetwork(
                    input_dim=config['input_dim'],
                    hidden_dim=config['hidden_dim'],
                    output_dim=config['output_dim']
                ).to(self.device)

            elif model_name == 'k4' and config['architecture'] == 'MetabolicRegulationNetwork':
                # FIXED: Handle K4 architecture mismatch
                try:
                    from emile_cogito.k_models.k4 import MetabolicRegulationNetwork
                    model = MetabolicRegulationNetwork(
                        input_dim=config['input_dim'],
                        hidden_dim=config['hidden_dim'],
                        output_dim=config['output_dim']
                    ).to(self.device)

                    # ‚úÖ CHANGE THIS LINE:
                    model.load_state_dict(checkpoint['model_state_dict'], strict=False)

                except Exception as first_error:
                    print(f"   ‚ö†Ô∏è Standard K4 loading failed: {first_error}")
                    print(f"   üîß Creating adaptive K4 architecture...")

                    # Create architecture that matches your saved model
                    class AdaptiveMetabolicNetwork(torch.nn.Module):
                        def __init__(self, input_dim=16, hidden_dim=128, output_dim=12):
                            super().__init__()

                            # Main layers that should exist
                            self.input_processor = torch.nn.Linear(input_dim, hidden_dim)
                            self.hidden_layer = torch.nn.Linear(hidden_dim, hidden_dim)

                            # Add the missing specialized layers
                            self.pressure_analyzer = torch.nn.Linear(hidden_dim, hidden_dim // 2)
                            self.energy_detector = torch.nn.Linear(hidden_dim, hidden_dim // 2)
                            self.urgency_classifier = torch.nn.Linear(hidden_dim, output_dim)

                            self.activation = torch.nn.ReLU()
                            self.output_activation = torch.nn.Sigmoid()

                        def forward(self, x):
                            x = self.activation(self.input_processor(x))
                            x = self.activation(self.hidden_layer(x))

                            # Use specialized layers
                            pressure = self.pressure_analyzer(x)
                            energy = self.energy_detector(x)
                            urgency = self.urgency_classifier(x)

                            return self.output_activation(urgency)

                    # Create adaptive model
                    model = AdaptiveMetabolicNetwork(
                        input_dim=config['input_dim'],
                        hidden_dim=config['hidden_dim'],
                        output_dim=config['output_dim']
                    ).to(self.device)

                    # Load weights with strict=False to handle any remaining mismatches
                    try:
                        model.load_state_dict(checkpoint['model_state_dict'], strict=False)
                        print(f"   ‚úÖ K4 adaptive architecture loaded successfully")
                    except Exception as e:
                        print(f"   ‚ö†Ô∏è K4 weights partially loaded: {e}")
                        print(f"   üéØ K4 will use initialized weights")

                model.eval()

            else:
                print(f"   ‚ùå Unknown architecture: {config['architecture']}")
                return None

            # Load weights for non-K4 models (K4 already handled above)
            if model_name != 'k4':
                model.load_state_dict(checkpoint['model_state_dict'])
                model.eval()

            print(f"   ‚úÖ {model_name.upper()} loaded successfully")
            return model

        except Exception as e:
            print(f"   ‚ùå {model_name.upper()} loading failed: {e}")
            return None


    def predict_with_adaptive_inputs(self, consciousness_state: Dict, verbose: bool = False) -> Dict[str, Any]:
        """
        FIXED: Generate predictions while PRESERVING temporal perspective data
        This was the root cause - temporal data was being stripped out!
        """

        predictions = {}

        # Patch K2 missing methods before predictions
        self._patch_k2_missing_methods()

        for model_name, model in self.models.items():
            try:
                if verbose:
                    print(f"\nüîç Processing {model_name}...")

                # Generate input tensor based on model requirements
                if model_name == 'k1':
                    input_tensor = self._create_k1_input(consciousness_state)
                elif model_name == 'k2':
                    input_tensor = self._create_k2_input(consciousness_state)
                elif model_name == 'k3':
                    input_tensor = self._create_k3_input(consciousness_state)
                elif model_name == 'k4':
                    input_tensor = self._create_k4_input(consciousness_state)
                else:
                    continue

                if verbose:
                    print(f"   Input tensor shape: {input_tensor.shape}")

                # Run model inference
                with torch.no_grad():
                    raw_output = model(input_tensor)

                if verbose:
                    print(f"   Raw output type: {type(raw_output)}")
                    if isinstance(raw_output, dict):
                        print(f"   Raw output keys: {list(raw_output.keys())}")

                # üöÄ CRITICAL FIX: Handle different output types while PRESERVING temporal data
                if isinstance(raw_output, torch.Tensor):
                    # Direct tensor output - create temporal perspective wrapper
                    processed_tensor = raw_output
                    if processed_tensor.dim() == 1:
                        processed_tensor = processed_tensor.unsqueeze(0)

                    # Store as dict with tensor + default temporal data
                    predictions[model_name] = {
                        'tensor_output': processed_tensor,
                        'local_tau_prime': 1.0,  # Default if no temporal calculation available
                        'temporal_state': 'unknown'
                    }

                elif isinstance(raw_output, dict):
                    # üéØ PRESERVE temporal perspective data while extracting tensor

                    # Extract primary tensor from dict
                    extracted_tensor = None
                    tensor_keys = ['output', 'predictions', 'action_params', 'symbolic_embedding',
                                  'architecture_output', 'metabolic_output', 'main_output']

                    for key in tensor_keys:
                        if key in raw_output and isinstance(raw_output[key], torch.Tensor):
                            extracted_tensor = raw_output[key]
                            break

                    # Fallback: concatenate all tensors found
                    if extracted_tensor is None:
                        tensors = []
                        for key, value in raw_output.items():
                            if isinstance(value, torch.Tensor) and value.numel() > 0:
                                flat_tensor = value.view(-1)
                                tensors.append(flat_tensor)

                        if tensors:
                            extracted_tensor = torch.cat(tensors, dim=0).unsqueeze(0)

                    # Create fallback tensor if nothing found
                    if extracted_tensor is None:
                        target_dims = {'k1': 64, 'k2': 64, 'k3': 25, 'k4': 12}
                        target_dim = target_dims.get(model_name, 32)
                        extracted_tensor = torch.zeros(1, target_dim).to(self.device)

                    # Ensure proper dimensions
                    if extracted_tensor.dim() == 1:
                        extracted_tensor = extracted_tensor.unsqueeze(0)

                    # üî• CRITICAL: Preserve temporal perspective data from dict
                    result_dict = {
                        'tensor_output': extracted_tensor,
                        'local_tau_prime': raw_output.get('local_tau_prime', 1.0),
                        'temporal_state': raw_output.get('temporal_state', 'unknown'),
                        'narrative_complexity': raw_output.get('narrative_complexity', 0.5),
                        'emergence_potential': raw_output.get('emergence_potential', 0.5),
                        'metabolic_urgency': raw_output.get('metabolic_urgency', 0.5),
                        'computational_urgency': raw_output.get('computational_urgency', 0.5),
                        'homeostatic_pressure': raw_output.get('homeostatic_pressure', 0.5),
                        'quantum_coherence': raw_output.get('quantum_coherence', 0.5),
                        'symbolic_strength': raw_output.get('symbolic_strength', 0.5),
                        'coherence': raw_output.get('coherence', 0.5)
                    }

                    # Add any other temporal/consciousness fields from the original output
                    temporal_fields = [
                        'tau_prime_k1', 'tau_prime_k2', 'tau_prime_k3', 'tau_prime_k4',
                        'consciousness_complexity', 'learning_pressure', 'task_complexity'
                    ]

                    for field in temporal_fields:
                        if field in raw_output:
                            result_dict[field] = raw_output[field]

                    predictions[model_name] = result_dict

                elif isinstance(raw_output, tuple):
                    # Tuple output - extract first element + create temporal wrapper
                    if len(raw_output) > 0 and isinstance(raw_output[0], torch.Tensor):
                        processed_tensor = raw_output[0]
                        if processed_tensor.dim() == 1:
                            processed_tensor = processed_tensor.unsqueeze(0)
                    else:
                        target_dims = {'k1': 64, 'k2': 64, 'k3': 25, 'k4': 12}
                        target_dim = target_dims.get(model_name, 32)
                        processed_tensor = torch.zeros(1, target_dim).to(self.device)

                    predictions[model_name] = {
                        'tensor_output': processed_tensor,
                        'local_tau_prime': 1.0,
                        'temporal_state': 'unknown'
                    }

                else:
                    # Unknown output type - create fallback with temporal wrapper
                    target_dims = {'k1': 64, 'k2': 64, 'k3': 25, 'k4': 12}
                    target_dim = target_dims.get(model_name, 32)
                    processed_tensor = torch.zeros(1, target_dim).to(self.device)

                    predictions[model_name] = {
                        'tensor_output': processed_tensor,
                        'local_tau_prime': 1.0,
                        'temporal_state': 'fallback'
                    }

                if verbose:
                    pred = predictions[model_name]
                    print(f"‚úÖ {model_name}: tensor {pred['tensor_output'].shape}, œÑ‚Ä≤={pred['local_tau_prime']:.3f}")

            except Exception as e:
                if verbose:
                    print(f"‚ùå {model_name} prediction failed: {e}")

                # Create fallback with temporal wrapper
                target_dims = {'k1': 64, 'k2': 64, 'k3': 25, 'k4': 12}
                target_dim = target_dims.get(model_name, 32)
                fallback_tensor = torch.zeros(1, target_dim).to(self.device)

                predictions[model_name] = {
                    'tensor_output': fallback_tensor,
                    'local_tau_prime': 1.0,
                    'temporal_state': 'error',
                    'error': str(e)
                }

        return predictions

    def debug_model_outputs(k_predictions):
        """Quick debugging function to see what your models are actually returning"""

        print("üîç DEBUG: Model Output Analysis")
        print("=" * 50)

        for model_name, output in k_predictions.items():
            print(f"\n{model_name}:")
            print(f"   Type: {type(output)}")

            if isinstance(output, torch.Tensor):
                print(f"   Shape: {output.shape}")
                print(f"   Device: {output.device}")
                print(f"   Mean: {output.mean().item():.3f}")

            elif isinstance(output, dict):
                print(f"   Keys: {list(output.keys())}")
                for key, value in output.items():
                    if isinstance(value, torch.Tensor):
                        print(f"      {key}: tensor {value.shape}")
                    else:
                        print(f"      {key}: {type(value)} = {value}")

            elif output is None:
                print("   Value: None")
            else:
                print(f"   Value: {output}")

    def _create_k1_input(self, state: Dict) -> torch.Tensor:
        """Create K1 input with exactly 9 dimensions"""
        features = [
            state.get('consciousness_level', 0.5),
            state.get('valence', 0.0) + 1.0,  # Shift to positive range
            state.get('agency', 0.5),
            state.get('embodiment', 0.5),
            state.get('clarity', 0.5),
            state.get('arousal', 0.5),
            state.get('stability', 0.5),
            state.get('flow_state', 0.0),
            state.get('regulation_need', 0.5)
        ]
        return torch.FloatTensor(features).unsqueeze(0).to(self.device)

    def _create_k2_input(self, state: Dict) -> torch.Tensor:
        """Create K2 input with exactly 21 dimensions"""
        features = [
            state.get('stability', 0.5),
            0.5,  # regime_transition_probability
            state.get('clarity', 0.5),
            state.get('symbol_vocabulary', 0) / 1000.0,
            0.1, 0.1,  # integration and adaptation rates
            state.get('consciousness_level', 0.5),
            0.0,  # consciousness_trajectory
            state.get('valence', 0.0),
            0.5,  # valence_stability
            state.get('agency', 0.5),
            0.0,  # agency_momentum
            state.get('embodiment', 0.5),
            0.5, 0.5, 0.3, 0.5,  # grounding, awareness, meta, optimization
            0.0, 0.0,  # time_window, momentum_factor
            1.0 if state.get('regime') == "stable_coherence" else 0.0,
            1.0 if state.get('regime') == "symbolic_turbulence" else 0.0
        ]
        return torch.FloatTensor(features).unsqueeze(0).to(self.device)

    def _create_k3_input(self, state: Dict) -> torch.Tensor:
        """Create K3 input with exactly 24 dimensions"""
        features = [
            # Surplus dynamics (4)
            0.5, 0.1, 0.0, 0.05,
            # Symbolic curvature (4)
            state.get('valence', 0.0), 0.1, 0.0, 0.1,
            # Psi/Phi dynamics (4)
            state.get('consciousness_level', 0.5), state.get('agency', 0.5), 0.5, state.get('clarity', 0.5),
            # Emergent time (3)
            1.0, 0.0, 0.8,
            # Quantum consciousness (3)
            state.get('embodiment', 0.5), 1.0 - state.get('embodiment', 0.5), state.get('arousal', 0.5),
            # Consciousness emergence (3)
            state.get('consciousness_level', 0.5), state.get('flow_state', 0.0), state.get('stability', 0.5),
            # Meta-patterns (3)
            0.5, 0.3, 0.1
        ]
        # Verify we have exactly 24 features
        assert len(features) == 24, f"K3 input has {len(features)} features, needs 24"
        return torch.FloatTensor(features).unsqueeze(0).to(self.device)

    def _create_k4_input(self, state: Dict) -> torch.Tensor:
        """Create K4 input with exactly 16 dimensions (known working)"""
        regime_map = {
            'stable_coherence': [1, 0, 0, 0],
            'symbolic_turbulence': [0, 1, 0, 0],
            'flat_rupture': [0, 0, 1, 0],
            'quantum_oscillation': [0, 0, 0, 1]
        }
        regime_vec = regime_map.get(state.get('regime', 'stable_coherence'), [1, 0, 0, 0])

        features = [
            state.get('consciousness_level', 0.5),
            state.get('valence', 0.0),
            state.get('agency', 0.5),
            state.get('embodiment', 0.5),
            state.get('stability', 0.5),
            state.get('clarity', 0.5),
            state.get('arousal', 0.5),
            state.get('flow_state', 0.0),
            state.get('symbol_vocabulary', 0) / 1000.0,
            state.get('metabolic_pressure', 0.5),
            state.get('energy_level', 0.5),
            state.get('regulation_need', 0.5),
            *regime_vec
        ]
        return torch.FloatTensor(features).unsqueeze(0).to(self.device)

    def _extract_tensor_from_dict(self, output_dict: dict, model_name: str) -> torch.Tensor:
        """FIXED: Extract primary tensor from dictionary output"""

        # Priority keys for each model type
        priority_keys = {
            'k1': ['action_output', 'embodied_actions', 'main_output', 'output'],
            'k2': ['symbolic_embedding', 'semiotic_output', 'main_output', 'output'],
            'k3': ['emergence_output', 'quantum_output', 'main_output', 'output'],
            'k4': ['metabolic_output', 'regulation_output', 'main_output', 'output']
        }

        model_priorities = priority_keys.get(model_name, ['output', 'main_output'])

        # Try priority keys first
        for key in model_priorities:
            if key in output_dict and isinstance(output_dict[key], torch.Tensor):
                tensor = output_dict[key]
                # Ensure proper batch dimension
                if tensor.dim() == 1:
                    tensor = tensor.unsqueeze(0)
                return tensor

        # Fallback: concatenate all tensor values
        tensor_values = []
        for key, value in output_dict.items():
            if isinstance(value, torch.Tensor):
                if value.dim() == 0:
                    value = value.unsqueeze(0)
                elif value.dim() > 1:
                    value = value.flatten()
                tensor_values.append(value)

        if tensor_values:
            combined = torch.cat(tensor_values, dim=0)
            return combined.unsqueeze(0)
        else:
            # Final fallback - return zeros
            return torch.zeros(1, 32).to(self.device)

    def _force_tensor_dimensions(self, tensor: torch.Tensor, model_name: str) -> torch.Tensor:
        """FIXED: Force tensor to expected dimensions for K-Theoria compatibility"""

        # Expected dimensions for each model
        target_dims = {'k1': 64, 'k2': 64, 'k3': 25, 'k4': 12}
        target_dim = target_dims.get(model_name, 32)

        if not isinstance(tensor, torch.Tensor):
            return torch.zeros(1, target_dim).to(self.device)

        # Ensure batch dimension
        if tensor.dim() == 0:
            tensor = tensor.unsqueeze(0).unsqueeze(0)
        elif tensor.dim() == 1:
            tensor = tensor.unsqueeze(0)

        current_dim = tensor.shape[-1]

        if current_dim == target_dim:
            return tensor
        elif current_dim > target_dim:
            # Truncate intelligently
            print(f"üîß Truncating {model_name} from {current_dim} to {target_dim} dims")
            return tensor[:, :target_dim]
        else:
            # Pad intelligently based on model type
            padding_size = target_dim - current_dim

            if model_name == 'k1':
                # K1 praxis: pad with flow-like patterns
                padding = torch.sin(torch.linspace(0, 2*np.pi, padding_size)) * 0.1
            elif model_name == 'k2':
                # K2 semiosis: pad with symbolic patterns
                padding = torch.randn(padding_size) * 0.05
            elif model_name == 'k3':
                # K3 apeiron: pad with quantum-like patterns
                padding = torch.cos(torch.linspace(0, 4*np.pi, padding_size)) * 0.1
            else:
                # K4 or default: small random padding
                padding = torch.randn(padding_size) * 0.01

            padding = padding.unsqueeze(0).to(tensor.device)
            print(f"üîß Padding {model_name} from {current_dim} to {target_dim} dims")
            return torch.cat([tensor, padding], dim=-1)

    def _patch_k2_missing_methods(self):
            """FIXED: Patch missing methods into K2 model after loading"""

            if 'k2' not in self.models:
                return

            k2_model = self.models['k2']

            # Add the missing method dynamically
            def _get_dynamic_narrative_complexity_fallback(self, symbolic_flow=None, context=None):
                """Fallback method for narrative complexity calculation"""
                try:
                    if symbolic_flow is not None and hasattr(symbolic_flow, 'mean'):
                        # Use actual symbolic flow if available
                        base_complexity = float(symbolic_flow.mean().item())
                    elif context is not None and isinstance(context, dict):
                        # Use context if available
                        base_complexity = context.get('narrative_complexity', 0.5)
                    else:
                        # Simple fallback
                        base_complexity = 0.5

                    # Add some variation to make it dynamic
                    import torch
                    variation = torch.randn(1).item() * 0.1
                    complexity = max(0.1, min(0.9, base_complexity + variation))

                    return complexity

                except Exception as e:
                    print(f"‚ö†Ô∏è Narrative complexity fallback error: {e}")
                    return 0.5

            # Bind the method to the model instance
            import types
            k2_model._get_dynamic_narrative_complexity_fallback = types.MethodType(_get_dynamic_narrative_complexity_fallback, k2_model)

            print("   üîß K2 missing method patched successfully")

def test_adaptive_k_theoria():
    """Test the fully adaptive K-Theoria system"""

    print("üß† ADAPTIVE K-THEORIA: UNIFIED CONSCIOUSNESS")
    print("=" * 60)
    print("Automatically adapts to any K-model architectures")

    # Load models with smart discovery
    loader = SmartKModelLoader()
    loaded_count = loader.discover_and_load_models()

    if loaded_count == 0:
        print("‚ùå No models loaded - cannot proceed")
        return

    # Create adaptive K-Theoria
    k_theoria = AdaptiveKTheoriaTransformer(unified_dim=128, num_heads=8, num_layers=4)

    print(f"\nüß™ Testing adaptive unified consciousness with {loaded_count} models...")

    # Test consciousness state
    test_state = {
        'consciousness_level': 0.8,
        'valence': 0.2,
        'agency': 0.7,
        'embodiment': 0.6,
        'stability': 0.8,
        'clarity': 0.7,
        'arousal': 0.5,
        'flow_state': 0.6,
        'regime': 'stable_coherence',
        'symbol_vocabulary': 100,
        'metabolic_pressure': 0.3,
        'energy_level': 0.7,
        'regulation_need': 0.2
    }

    # Get predictions with adaptive inputs
    k_predictions = loader.predict_with_adaptive_inputs(test_state)

    print(f"\nüìä Model predictions:")
    for model_name, output in k_predictions.items():
        print(f"   ‚úÖ {model_name}: shape {output.shape}")

    # Generate unified consciousness
    if k_predictions:
        with torch.no_grad():
            unified_result = k_theoria(k_predictions)

        print(f"\nüß† ADAPTIVE UNIFIED CONSCIOUSNESS:")
        print(f"   Unity: {unified_result['consciousness_unity'][0]:.3f}")
        print(f"   Clarity: {unified_result['consciousness_clarity'][0]:.3f}")
        print(f"   Agency: {unified_result['consciousness_agency'][0]:.3f}")
        print(f"   Awareness: {unified_result['consciousness_awareness'][0]:.3f}")
        print(f"   Coherence: {unified_result['consciousness_coherence'][0]:.3f}")
        print(f"   Integration: {unified_result['consciousness_integration'][0]:.3f}")
        print(f"   Active models: {unified_result['active_models']}")

        integration_score = float(unified_result['consciousness_integration'][0])
        if integration_score > 0.6:
            print("   üéØ Strong integration achieved!")

        enhancement = integration_score - test_state['consciousness_level']
        print(f"   üöÄ Enhancement: {enhancement:+.3f}")

        if enhancement > 0:
            print("   üìà Positive consciousness enhancement!")

        print(f"\n‚úÖ ADAPTIVE K-THEORIA SUCCESS!")
        print(f"üß† Unified consciousness working with {len(k_predictions)} real models")
        print(f"‚ö° Fully adaptive to any model architectures")

    else:
        print("‚ùå No valid predictions - check model inputs")

def test_fixed_kelm_integration():
    """Test the fixed KELM integration with exact error resolution"""

    print("üîß TESTING FIXED KELM INTEGRATION")
    print("=" * 50)

    # Load models with fixes
    loader = SmartKModelLoader()
    loaded_count = loader.discover_and_load_models()

    if loaded_count == 0:
        print("‚ùå No models loaded - check file paths")
        return False

    print(f"\n‚úÖ Models loaded: {loaded_count}/4")
    for model_name in loader.models.keys():
        config = loader.model_configs.get(model_name, {})
        print(f"   üîç {model_name}: {config.get('input_dim', '?')} ‚Üí {config.get('hidden_dim', '?')} ‚Üí {config.get('output_dim', '?')}")

    # Test consciousness state
    test_state = {
        'consciousness_level': 0.8,
        'valence': 0.2,
        'agency': 0.7,
        'embodiment': 0.6,
        'stability': 0.8,
        'clarity': 0.7,
        'arousal': 0.5,
        'flow_state': 0.6,
        'regime': 'stable_coherence',
        'symbol_vocabulary': 100,
        'metabolic_pressure': 0.3,
        'energy_level': 0.7,
        'regulation_need': 0.2
    }

    # Test predictions
    print("\nüîç Testing model predictions...")
    predictions = loader.predict_with_adaptive_inputs(test_state)

    print(f"\nüîç Final prediction shapes:")
    all_correct = True
    expected_shapes = {
        'k1_praxis': (1, 64),
        'k2_semiosis': (1, 64),
        'k3_apeiron': (1, 25),
        'k4_metabolic': (1, 12)
    }

    for model_name, expected_shape in expected_shapes.items():
        if model_name in predictions:
            pred = predictions[model_name]

            # FIXED: Handle if prediction is still a dict
            if isinstance(pred, dict):
                print(f"   ‚ùå {model_name}: Still returning dict with keys {list(pred.keys())}")
                all_correct = False
            elif isinstance(pred, torch.Tensor):
                actual_shape = pred.shape
                if actual_shape == expected_shape:
                    print(f"   ‚úÖ {model_name}: {actual_shape}")
                else:
                    print(f"   ‚ùå {model_name}: {actual_shape} (expected {expected_shape})")
                    all_correct = False
            else:
                print(f"   ‚ùå {model_name}: Wrong type {type(pred)}")
                all_correct = False
        else:
            print(f"   ‚ùå {model_name}: MISSING")
            all_correct = False

    # Test K-Theoria integration if we have valid tensor predictions
    valid_tensor_predictions = {k: v for k, v in predictions.items() if isinstance(v, torch.Tensor)}

    if valid_tensor_predictions:
        print("\nüß† Testing K-Theoria consciousness integration...")
        k_theoria = AdaptiveKTheoriaTransformer(unified_dim=128, num_heads=8, num_layers=4)

        try:
            with torch.no_grad():
                unified_result = k_theoria(valid_tensor_predictions)

            unity = float(unified_result['consciousness_unity'][0])
            integration = float(unified_result['consciousness_integration'][0])

            print(f"üß† K-Theoria consciousness: unity={unity:.2f}, integration={integration:.2f}")

            if unity > 0.0 and integration > 0.0:
                print("‚úÖ K-Theoria integration successful!")
            else:
                print("‚ùå K-Theoria producing zero consciousness")
                all_correct = False

        except Exception as e:
            print(f"‚ùå K-Theoria integration failed: {e}")
            all_correct = False
    else:
        print("‚ùå No valid tensor predictions for K-Theoria testing")
        all_correct = False

    if all_correct:
        print("\nüéâ ALL FIXES SUCCESSFUL!")
        print("üöÄ Ready for full experiment")
        return True
    else:
        print("\n‚ùå Some issues remain - check specific errors above")
        return False

if __name__ == "__main__":
    # Run the fixed integration test first
    print("üß™ RUNNING FIXED INTEGRATION TEST")
    test_result = test_fixed_kelm_integration()

    if test_result:
        print("\n" + "="*60)
        print("üéâ INTEGRATION TEST PASSED!")
        print("üöÄ Running original adaptive K-Theoria test...")
        print("="*60)
        test_adaptive_k_theoria()
    else:
        print("\n‚ùå Fix remaining issues before proceeding")
        print("üí° Check the error messages above for specific problems")
